{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a82c478",
   "metadata": {},
   "source": [
    "# DA5401 A6: Imputation via Regression for Missing Data\n",
    "## Objective: \n",
    "\n",
    "This assignment challenges you to apply linear and non-linear regression to impute\n",
    "missing values in a dataset. The effectiveness of your imputation methods will be measured\n",
    "indirectly by assessing the performance of a subsequent classification task, comparing the\n",
    "regression-based approach against simpler imputation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0685f",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "You are a machine learning engineer working on a credit risk assessment project. You have\n",
    "been provided with the UCI Credit Card Default Clients Dataset. This dataset has missing\n",
    "values in several important feature columns. The presence of missing data prevents the\n",
    "immediate application of many classification algorithms.\n",
    "Your task is to implement three different strategies for handling the missing data and then use\n",
    "the resulting clean datasets to train and evaluate a classification model. This will demonstrate\n",
    "how the choice of imputation technique significantly impacts final model performance.\n",
    "You will submit a Jupyter Notebook with your complete code, visualizations, and a plausible\n",
    "story that explains your findings. The notebook should be well-commented, reproducible, and\n",
    "easy to follow.\n",
    "### Dataset:\n",
    "- UCI Credit Card Default Clients Dataset (with missing values): Kaggle - Credit Card\n",
    "Default Clients Dataset (https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset)\n",
    "    - Note: While the original UCI dataset is relatively clean, for this assignment, you\n",
    "should artificially introduce Missing At Random (MAR) values (e.g., replace\n",
    "5% of the values in the 'AGE' and 'BILL_AMT' columns with NaN) before starting\n",
    "Part A, to simulate a real-world scenario with a substantial missing data problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e91f9f",
   "metadata": {},
   "source": [
    "## 2. Tasks\n",
    "### Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3b13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534b4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (30000, 25)\n",
      "                AGE      BILL_AMT1\n",
      "count  30000.000000   30000.000000\n",
      "mean      35.485500   51223.330900\n",
      "std        9.217904   73635.860576\n",
      "min       21.000000 -165580.000000\n",
      "25%       28.000000    3558.750000\n",
      "50%       34.000000   22381.500000\n",
      "75%       41.000000   67091.000000\n",
      "max       79.000000  964511.000000\n",
      "\n",
      "Adjusted missing percentage after MAR injection:\n",
      "AGE          5.03\n",
      "BILL_AMT1    4.99\n",
      "dtype: float64\n",
      "\n",
      "Feature matrix and target vector created successfully.\n",
      "X shape: (30000, 24)\n",
      "y shape: (30000,)\n",
      "\n",
      "Target variable distribution:\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Prepare Data [4]: Load the dataset and, as instructed in the note above, artificially introduce MAR missing values (5-10% in 2-3 numerical feature columns). The target variable is 'default payment next month'\n",
    "\n",
    "df = pd.read_csv('/Users/navaneethakrishnan/Desktop/DAL/assignment_6_Navaneeth272001/UCI_Credit_Card.csv') #please change the path accordingly\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(df[['AGE', 'BILL_AMT1']].describe())\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1️⃣ Compute conditional subsets\n",
    "cond_bill = df['AGE'] > 50\n",
    "cond_age = df['EDUCATION'] == 1\n",
    "\n",
    "# 2️⃣ Calculate fraction needed for ~5% overall missing\n",
    "frac_bill = 0.05 / cond_bill.mean()\n",
    "frac_age = 0.05 / cond_age.mean()\n",
    "\n",
    "# 3️⃣ Apply MAR missingness scaled to achieve ~5% total\n",
    "mask_mar_bill = cond_bill & (np.random.rand(len(df)) < frac_bill)\n",
    "mask_mar_age = cond_age & (np.random.rand(len(df)) < frac_age)\n",
    "\n",
    "df.loc[mask_mar_bill, 'BILL_AMT1'] = np.nan\n",
    "df.loc[mask_mar_age, 'AGE'] = np.nan\n",
    "\n",
    "# Check result again\n",
    "missing_summary = df[['AGE', 'BILL_AMT1']].isna().mean() * 100\n",
    "print(\"\\nAdjusted missing percentage after MAR injection:\")\n",
    "print(missing_summary.round(2))\n",
    "\n",
    "# Define target column\n",
    "target_col = \"default.payment.next.month\"\n",
    "\n",
    "# Create target variable\n",
    "y = df[target_col]\n",
    "\n",
    "# Create feature matrix by dropping the target column\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# Confirm shapes\n",
    "print(\"\\nFeature matrix and target vector created successfully.\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(y.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50ce6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with missing values: ['AGE', 'BILL_AMT1']\n",
      "\n",
      "Missing values after median imputation:\n",
      "AGE          0\n",
      "BILL_AMT1    0\n",
      "dtype: int64\n",
      "\n",
      "Shape of Dataset A: (30000, 25)\n",
      "                AGE      BILL_AMT1\n",
      "count  30000.000000   30000.000000\n",
      "mean      35.483300   49635.322700\n",
      "std        9.034407   71505.228811\n",
      "min       21.000000 -165580.000000\n",
      "25%       28.000000    4150.250000\n",
      "50%       34.000000   22399.000000\n",
      "75%       41.000000   62774.250000\n",
      "max       79.000000  964511.000000\n"
     ]
    }
   ],
   "source": [
    "# 2. Imputation Strategy 1: Simple Imputation (Baseline):\n",
    "\n",
    "#Create a clean dataset copy (Dataset A). For each column with missing values, fill the missing values with the median of that column. \n",
    "# 3️⃣ Create a clean dataset copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 4️⃣ Identify columns with missing values\n",
    "missing_cols = df_clean.columns[df_clean.isna().any()]\n",
    "print(\"\\nColumns with missing values:\", list(missing_cols))\n",
    "\n",
    "# 5️⃣ Fill missing values with column median (Dataset A)\n",
    "df_A = df_clean.copy()\n",
    "for col in missing_cols:\n",
    "    median_value = df_A[col].median()\n",
    "    df_A[col] = df_A[col].fillna(median_value)\n",
    "\n",
    "# 6️⃣ Verify that all missing values are handled\n",
    "print(\"\\nMissing values after median imputation:\")\n",
    "print(df_A[missing_cols].isna().sum())\n",
    "\n",
    "# Optional: check if dataset shape and stats remain consistent\n",
    "print(\"\\nShape of Dataset A:\", df_A.shape)\n",
    "print(df_A[['AGE', 'BILL_AMT1']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7d2ff",
   "metadata": {},
   "source": [
    "Explain why the median is often preferred over the mean for imputation.\n",
    "\n",
    "- Robustness to Outliers\n",
    "    - The mean is sensitive to extreme values (outliers).\n",
    "    - Example: if one person has a bill amount of ₹10,00,000 while others have around ₹10,000, the mean will be pulled upward.\n",
    "    - The median, on the other hand, is not affected by outliers — it only depends on the middle value of the sorted data.\n",
    "    - So it provides a more stable and representative imputation value when the data are skewed or contain outliers.\n",
    "- Better for Skewed Distributions\n",
    "    - Many real-world variables (e.g., income, bill amounts, age) are right-skewed — meaning there are a few very large values.\n",
    "    - The mean in such cases doesn’t represent the “typical” observation, but the median does.\n",
    "    - Median imputation preserves the central tendency better for non-normal (skewed) distributions.\n",
    "- Preserves Rank and Spread Better\n",
    "    - When you impute using the mean, you might flatten variability and distort the data’s distribution.\n",
    "    - Using the median preserves relative ordering and the shape of the distribution more faithfully.\n",
    "- Simplicity and Interpretability\n",
    "    - Median imputation is simple, quick, and computationally efficient.\n",
    "    - It doesn’t require complex modeling assumptions — it just replaces missing values with a robust measure of central tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985af147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing rows in 'BILL_AMT1': 1497\n",
      "\n",
      "Missing values after Linear Regression imputation:\n",
      "0\n",
      "\n",
      "Shape of Dataset B: (30000, 25)\n",
      "           BILL_AMT1\n",
      "count   30000.000000\n",
      "mean    51235.041649\n",
      "std     73384.538489\n",
      "min   -165580.000000\n",
      "25%      3933.000000\n",
      "50%     22460.291531\n",
      "75%     66979.750000\n",
      "max    964511.000000\n"
     ]
    }
   ],
   "source": [
    "#3. Imputation Strategy 2: Regression Imputation (Linear):\n",
    "\n",
    "#Create a second clean dataset copy (Dataset B). For a single column (your choice) with missing values, use a Linear Regression model to predict the missing values based on all other non-missing features.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Make a fresh copy\n",
    "df_B = df.copy()\n",
    "\n",
    "# Choose the column with missing values (example: 'BILL_AMT1')\n",
    "target_col_missing = 'BILL_AMT1'\n",
    "\n",
    "# Split data into rows with and without missing target values\n",
    "df_not_missing = df_B[df_B[target_col_missing].notna()]\n",
    "df_missing = df_B[df_B[target_col_missing].isna()]\n",
    "\n",
    "print(f\"\\nNumber of missing rows in '{target_col_missing}': {len(df_missing)}\")\n",
    "\n",
    "# Define features (all other columns) and target\n",
    "X_train = df_not_missing.drop(columns=[target_col_missing])\n",
    "y_train = df_not_missing[target_col_missing]\n",
    "\n",
    "# Prepare rows where target_col_missing is NaN\n",
    "X_pred = df_missing.drop(columns=[target_col_missing])\n",
    "\n",
    "# Fill remaining missing numeric values (excluding the target column) with medians\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_pred = X_pred.fillna(X_pred.median())\n",
    "\n",
    "# Fit Linear Regression model on all available (non-missing) data\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing BILL_AMT1 values\n",
    "predicted_values = reg.predict(X_pred)\n",
    "\n",
    "# Replace missing values with predicted values\n",
    "df_B.loc[df_B[target_col_missing].isna(), target_col_missing] = predicted_values\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"\\nMissing values after Linear Regression imputation:\")\n",
    "print(df_B[target_col_missing].isna().sum())\n",
    "\n",
    "print(\"\\nShape of Dataset B:\", df_B.shape)\n",
    "print(df_B[[target_col_missing]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ab7d4",
   "metadata": {},
   "source": [
    "Explain the underlying assumption of this method (Missing At Random).\n",
    "\n",
    "- Missing At Random (MAR) Definition:\n",
    "    - Missingness of a variable depends on other observed variables, but not on the value of the variable itself.\n",
    "    - Mathematically:\n",
    "        - P(missing in X_miss | X_miss, X_obs) = P(missing in X_miss | X_obs)\n",
    "- How it relates to Linear Regression Imputation:\n",
    "    - Missing values can be predicted using correlations with observed features.\n",
    "    - Works well under MAR because missingness is systematically related to other variables, not the missing values themselves.\n",
    "    - Regression predicts the conditional mean of the missing values given observed variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c658833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing rows in 'BILL_AMT1': 1497\n",
      "\n",
      "Missing values after non-linear regression imputation:\n",
      "0\n",
      "\n",
      "Shape of Dataset C: (30000, 25)\n",
      "           BILL_AMT1\n",
      "count   30000.000000\n",
      "mean    51138.641393\n",
      "std     73273.520809\n",
      "min   -165580.000000\n",
      "25%      3699.250000\n",
      "50%     22430.500000\n",
      "75%     66982.000000\n",
      "max    964511.000000\n"
     ]
    }
   ],
   "source": [
    "# 4. Imputation Strategy 3: Regression Imputation (Non-Linear):\n",
    "\n",
    "#Create a third clean dataset copy (Dataset C). For the same column as in Strategy 2, use a non-linear regression model (e.g., K-Nearest Neighbors Regression or Decision Tree Regression) to predict the missing values.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Make a fresh copy\n",
    "df_C = df.copy()\n",
    "\n",
    "# Column with missing values\n",
    "target_col_missing = 'BILL_AMT1'\n",
    "\n",
    "# Split data into rows with and without missing target values\n",
    "df_not_missing = df_C[df_C[target_col_missing].notna()]\n",
    "df_missing = df_C[df_C[target_col_missing].isna()]\n",
    "\n",
    "print(f\"\\nNumber of missing rows in '{target_col_missing}': {len(df_missing)}\")\n",
    "\n",
    "# Define features (all other columns) and target\n",
    "X_train = df_not_missing.drop(columns=[target_col_missing])\n",
    "y_train = df_not_missing[target_col_missing]\n",
    "\n",
    "# Prepare rows where target_col_missing is NaN\n",
    "X_pred = df_missing.drop(columns=[target_col_missing])\n",
    "\n",
    "# Fill remaining missing numeric values (excluding the target column) with medians\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_pred = X_pred.fillna(X_pred.median())\n",
    "\n",
    "# --- Choose one non-linear regression model ---\n",
    "# Option 1: K-Nearest Neighbors Regressor\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Option 2: Decision Tree Regressor (uncomment the next line to use it instead)\n",
    "# model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "# Fit model on all available (non-missing) data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing BILL_AMT1 values\n",
    "predicted_values = model.predict(X_pred)\n",
    "\n",
    "# Replace missing values with predicted values\n",
    "df_C.loc[df_C[target_col_missing].isna(), target_col_missing] = predicted_values\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"\\nMissing values after non-linear regression imputation:\")\n",
    "print(df_C[target_col_missing].isna().sum())\n",
    "\n",
    "print(\"\\nShape of Dataset C:\", df_C.shape)\n",
    "print(df_C[[target_col_missing]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de829cd",
   "metadata": {},
   "source": [
    "### Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Split: For each of the three imputed datasets (A, B, C), split the data into training and testing sets. Also, create a fourth dataset (Dataset D) by simply removing all rows that contain any missing values (Listwise Deletion). Split Dataset D into training and testing sets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
