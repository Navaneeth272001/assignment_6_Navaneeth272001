{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a82c478",
   "metadata": {},
   "source": [
    "# DA5401 A6: Imputation via Regression for Missing Data\n",
    "## Objective: \n",
    "\n",
    "This assignment challenges you to apply linear and non-linear regression to impute\n",
    "missing values in a dataset. The effectiveness of your imputation methods will be measured\n",
    "indirectly by assessing the performance of a subsequent classification task, comparing the\n",
    "regression-based approach against simpler imputation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0685f",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "You are a machine learning engineer working on a credit risk assessment project. You have\n",
    "been provided with the UCI Credit Card Default Clients Dataset. This dataset has missing\n",
    "values in several important feature columns. The presence of missing data prevents the\n",
    "immediate application of many classification algorithms.\n",
    "Your task is to implement three different strategies for handling the missing data and then use\n",
    "the resulting clean datasets to train and evaluate a classification model. This will demonstrate\n",
    "how the choice of imputation technique significantly impacts final model performance.\n",
    "You will submit a Jupyter Notebook with your complete code, visualizations, and a plausible\n",
    "story that explains your findings. The notebook should be well-commented, reproducible, and\n",
    "easy to follow.\n",
    "### Dataset:\n",
    "- UCI Credit Card Default Clients Dataset (with missing values): Kaggle - Credit Card\n",
    "Default Clients Dataset (https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset)\n",
    "    - Note: While the original UCI dataset is relatively clean, for this assignment, you\n",
    "should artificially introduce Missing At Random (MAR) values (e.g., replace\n",
    "5% of the values in the 'AGE' and 'BILL_AMT' columns with NaN) before starting\n",
    "Part A, to simulate a real-world scenario with a substantial missing data problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e91f9f",
   "metadata": {},
   "source": [
    "## 2. Tasks\n",
    "### Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3b13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "534b4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (30000, 25)\n",
      "                AGE      BILL_AMT1      PAY_AMT3\n",
      "count  30000.000000   30000.000000   30000.00000\n",
      "mean      35.485500   51223.330900    5225.68150\n",
      "std        9.217904   73635.860576   17606.96147\n",
      "min       21.000000 -165580.000000       0.00000\n",
      "25%       28.000000    3558.750000     390.00000\n",
      "50%       34.000000   22381.500000    1800.00000\n",
      "75%       41.000000   67091.000000    4505.00000\n",
      "max       79.000000  964511.000000  896040.00000\n",
      "\n",
      "Adjusted missing percentage after MAR injection (df_mar):\n",
      "AGE          5.03\n",
      "BILL_AMT1    4.99\n",
      "PAY_AMT3     5.00\n",
      "dtype: float64\n",
      "\n",
      "Feature matrix and target vector created successfully (from df_mar).\n",
      "X shape: (30000, 24)\n",
      "y shape: (30000,)\n",
      "\n",
      "Target variable distribution:\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1Ô∏è‚É£ Load dataset\n",
    "df = pd.read_csv('/Users/navaneethakrishnan/Desktop/DAL/assignment_6_Navaneeth272001/UCI_Credit_Card.csv')  # change path if needed\n",
    "print(\"Initial shape:\", df.shape)\n",
    "print(df[['AGE', 'BILL_AMT1', 'PAY_AMT3']].describe())\n",
    "\n",
    "# 2Ô∏è‚É£ Create a copy for MAR missingness injection\n",
    "df_mar = df.copy()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 3Ô∏è‚É£ Compute conditional subsets (for MAR logic)\n",
    "cond_bill = df_mar['AGE'] > 50          # BILL_AMT1 depends on AGE\n",
    "cond_age = df_mar['EDUCATION'] == 1     # AGE depends on EDUCATION\n",
    "cond_pay = df_mar['MARRIAGE'] == 2      # PAY_AMT3 depends on MARRIAGE status (example)\n",
    "\n",
    "# 4Ô∏è‚É£ Calculate fractions needed for ~5% overall missing per feature\n",
    "frac_bill = 0.05 / cond_bill.mean()\n",
    "frac_age = 0.05 / cond_age.mean()\n",
    "frac_pay = 0.05 / cond_pay.mean()\n",
    "\n",
    "# 5Ô∏è‚É£ Apply MAR missingness scaled to achieve ~5% total for each column\n",
    "mask_mar_bill = cond_bill & (np.random.rand(len(df_mar)) < frac_bill)\n",
    "mask_mar_age = cond_age & (np.random.rand(len(df_mar)) < frac_age)\n",
    "mask_mar_pay = cond_pay & (np.random.rand(len(df_mar)) < frac_pay)\n",
    "\n",
    "df_mar.loc[mask_mar_bill, 'BILL_AMT1'] = np.nan\n",
    "df_mar.loc[mask_mar_age, 'AGE'] = np.nan\n",
    "df_mar.loc[mask_mar_pay, 'PAY_AMT3'] = np.nan\n",
    "\n",
    "# 6Ô∏è‚É£ Check result again\n",
    "missing_summary = df_mar[['AGE', 'BILL_AMT1', 'PAY_AMT3']].isna().mean() * 100\n",
    "print(\"\\nAdjusted missing percentage after MAR injection (df_mar):\")\n",
    "print(missing_summary.round(2))\n",
    "\n",
    "# 7Ô∏è‚É£ Define target column\n",
    "target_col = \"default.payment.next.month\"\n",
    "\n",
    "# 8Ô∏è‚É£ Create target variable\n",
    "y = df_mar[target_col]\n",
    "\n",
    "# 9Ô∏è‚É£ Create feature matrix by dropping the target column\n",
    "X = df_mar.drop(columns=[target_col])\n",
    "\n",
    "# üîü Confirm shapes\n",
    "print(\"\\nFeature matrix and target vector created successfully (from df_mar).\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50ce6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with missing values: ['AGE', 'BILL_AMT1', 'PAY_AMT3']\n",
      "\n",
      "Missing values after median imputation:\n",
      "AGE          0\n",
      "BILL_AMT1    0\n",
      "PAY_AMT3     0\n",
      "dtype: int64\n",
      "\n",
      "Shape of Dataset A: (30000, 25)\n",
      "                AGE      BILL_AMT1\n",
      "count  30000.000000   30000.000000\n",
      "mean      35.483300   49635.322700\n",
      "std        9.034407   71505.228811\n",
      "min       21.000000 -165580.000000\n",
      "25%       28.000000    4150.250000\n",
      "50%       34.000000   22399.000000\n",
      "75%       41.000000   62774.250000\n",
      "max       79.000000  964511.000000\n"
     ]
    }
   ],
   "source": [
    "# 2. Imputation Strategy 1: Simple Imputation (Baseline):\n",
    "\n",
    "#Create a clean dataset copy (Dataset A). For each column with missing values, fill the missing values with the median of that column. \n",
    "# 3Ô∏è‚É£ Create a clean dataset copy\n",
    "df_clean = df_mar.copy()\n",
    "\n",
    "# 4Ô∏è‚É£ Identify columns with missing values\n",
    "missing_cols = df_clean.columns[df_clean.isna().any()]\n",
    "print(\"\\nColumns with missing values:\", list(missing_cols))\n",
    "\n",
    "# 5Ô∏è‚É£ Fill missing values with column median (Dataset A)\n",
    "df_A = df_clean.copy()\n",
    "for col in missing_cols:\n",
    "    median_value = df_A[col].median()\n",
    "    df_A[col] = df_A[col].fillna(median_value)\n",
    "\n",
    "# 6Ô∏è‚É£ Verify that all missing values are handled\n",
    "print(\"\\nMissing values after median imputation:\")\n",
    "print(df_A[missing_cols].isna().sum())\n",
    "\n",
    "# Optional: check if dataset shape and stats remain consistent\n",
    "print(\"\\nShape of Dataset A:\", df_A.shape)\n",
    "print(df_A[['AGE', 'BILL_AMT1']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7d2ff",
   "metadata": {},
   "source": [
    "Explain why the median is often preferred over the mean for imputation.\n",
    "\n",
    "- Robustness to Outliers\n",
    "    - The mean is sensitive to extreme values (outliers).\n",
    "    - Example: if one person has a bill amount of ‚Çπ10,00,000 while others have around ‚Çπ10,000, the mean will be pulled upward.\n",
    "    - The median, on the other hand, is not affected by outliers ‚Äî it only depends on the middle value of the sorted data.\n",
    "    - So it provides a more stable and representative imputation value when the data are skewed or contain outliers.\n",
    "- Better for Skewed Distributions\n",
    "    - Many real-world variables (e.g., income, bill amounts, age) are right-skewed ‚Äî meaning there are a few very large values.\n",
    "    - The mean in such cases doesn‚Äôt represent the ‚Äútypical‚Äù observation, but the median does.\n",
    "    - Median imputation preserves the central tendency better for non-normal (skewed) distributions.\n",
    "- Preserves Rank and Spread Better\n",
    "    - When you impute using the mean, you might flatten variability and distort the data‚Äôs distribution.\n",
    "    - Using the median preserves relative ordering and the shape of the distribution more faithfully.\n",
    "- Simplicity and Interpretability\n",
    "    - Median imputation is simple, quick, and computationally efficient.\n",
    "    - It doesn‚Äôt require complex modeling assumptions ‚Äî it just replaces missing values with a robust measure of central tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce2766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjusted missing percentage after MAR injection (df_mar):\n",
      "BILL_AMT1    4.99\n",
      "dtype: float64\n",
      "\n",
      "Feature matrix and target vector created successfully (from df_mar).\n",
      "X shape: (30000, 24)\n",
      "y shape: (30000,)\n",
      "\n",
      "Target variable distribution:\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataset copy with one of column with nul value for training models\n",
    "df_mar_1 = df.copy()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 3Ô∏è‚É£ Define MAR condition for one feature (example: BILL_AMT1 depends on AGE)\n",
    "cond_bill = df_mar_1['AGE'] > 50\n",
    "\n",
    "# 4Ô∏è‚É£ Calculate fraction needed for ~5% overall missing\n",
    "frac_bill = 0.05 / cond_bill.mean()\n",
    "\n",
    "# 5Ô∏è‚É£ Apply MAR missingness scaled to achieve ~5% total\n",
    "mask_mar_bill = cond_bill & (np.random.rand(len(df_mar_1)) < frac_bill)\n",
    "df_mar_1.loc[mask_mar_bill, 'BILL_AMT1'] = np.nan\n",
    "\n",
    "# 6Ô∏è‚É£ Check result again\n",
    "missing_summary = df_mar_1[['BILL_AMT1']].isna().mean() * 100\n",
    "print(\"\\nAdjusted missing percentage after MAR injection (df_mar):\")\n",
    "print(missing_summary.round(2))\n",
    "\n",
    "# 7Ô∏è‚É£ Define target column\n",
    "target_col = \"default.payment.next.month\"\n",
    "\n",
    "# 8Ô∏è‚É£ Create target variable\n",
    "y = df_mar_1[target_col]\n",
    "\n",
    "# 9Ô∏è‚É£ Create feature matrix by dropping the target column\n",
    "X = df_mar_1.drop(columns=[target_col])\n",
    "\n",
    "# üîü Confirm shapes\n",
    "print(\"\\nFeature matrix and target vector created successfully (from df_mar).\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985af147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing rows in 'BILL_AMT1': 1497\n",
      "\n",
      "Missing values after Linear Regression imputation:\n",
      "0\n",
      "\n",
      "Shape of Dataset B: (30000, 25)\n",
      "           BILL_AMT1\n",
      "count   30000.000000\n",
      "mean    51234.990466\n",
      "std     73384.517925\n",
      "min   -165580.000000\n",
      "25%      3933.000000\n",
      "50%     22458.988339\n",
      "75%     66979.750000\n",
      "max    964511.000000\n"
     ]
    }
   ],
   "source": [
    "#3. Imputation Strategy 2: Regression Imputation (Linear):\n",
    "\n",
    "#Create a second clean dataset copy (Dataset B). For a single column (your choice) with missing values, use a Linear Regression model to predict the missing values based on all other non-missing features.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Make a fresh copy\n",
    "df_B = df_mar_1.copy()\n",
    "\n",
    "# Choose the column with missing values (example: 'BILL_AMT1')\n",
    "target_col_missing = 'BILL_AMT1'\n",
    "\n",
    "# Split data into rows with and without missing target values\n",
    "df_not_missing = df_B[df_B[target_col_missing].notna()]\n",
    "df_missing = df_B[df_B[target_col_missing].isna()]\n",
    "\n",
    "print(f\"\\nNumber of missing rows in '{target_col_missing}': {len(df_missing)}\")\n",
    "\n",
    "# Define features (all other columns) and target\n",
    "X_train = df_not_missing.drop(columns=[target_col_missing])\n",
    "y_train = df_not_missing[target_col_missing]\n",
    "\n",
    "# Prepare rows where target_col_missing is NaN\n",
    "X_pred = df_missing.drop(columns=[target_col_missing])\n",
    "\n",
    "# Fill remaining missing numeric values (excluding the target column) with medians\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_pred = X_pred.fillna(X_pred.median())\n",
    "\n",
    "# Fit Linear Regression model on all available (non-missing) data\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing BILL_AMT1 values\n",
    "predicted_values = reg.predict(X_pred)\n",
    "\n",
    "# Replace missing values with predicted values\n",
    "df_B.loc[df_B[target_col_missing].isna(), target_col_missing] = predicted_values\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"\\nMissing values after Linear Regression imputation:\")\n",
    "print(df_B[target_col_missing].isna().sum())\n",
    "\n",
    "print(\"\\nShape of Dataset B:\", df_B.shape)\n",
    "print(df_B[[target_col_missing]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ab7d4",
   "metadata": {},
   "source": [
    "Explain the underlying assumption of this method (Missing At Random).\n",
    "\n",
    "- Missing At Random (MAR) Definition:\n",
    "    - Missingness of a variable depends on other observed variables, but not on the value of the variable itself.\n",
    "    - Mathematically:\n",
    "        - P(missing in X_miss | X_miss, X_obs) = P(missing in X_miss | X_obs)\n",
    "- How it relates to Linear Regression Imputation:\n",
    "    - Missing values can be predicted using correlations with observed features.\n",
    "    - Works well under MAR because missingness is systematically related to other variables, not the missing values themselves.\n",
    "    - Regression predicts the conditional mean of the missing values given observed variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c658833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing rows in 'BILL_AMT1': 1497\n",
      "\n",
      "Missing values after non-linear regression imputation:\n",
      "0\n",
      "\n",
      "Shape of Dataset C: (30000, 25)\n",
      "           BILL_AMT1\n",
      "count   30000.000000\n",
      "mean    51138.641393\n",
      "std     73273.520809\n",
      "min   -165580.000000\n",
      "25%      3699.250000\n",
      "50%     22430.500000\n",
      "75%     66982.000000\n",
      "max    964511.000000\n"
     ]
    }
   ],
   "source": [
    "# 4. Imputation Strategy 3: Regression Imputation (Non-Linear):\n",
    "\n",
    "#Create a third clean dataset copy (Dataset C). For the same column as in Strategy 2, use a non-linear regression model (e.g., K-Nearest Neighbors Regression or Decision Tree Regression) to predict the missing values.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Make a fresh copy\n",
    "df_C = df_mar_1.copy()\n",
    "\n",
    "# Column with missing values\n",
    "target_col_missing = 'BILL_AMT1'\n",
    "\n",
    "# Split data into rows with and without missing target values\n",
    "df_not_missing = df_C[df_C[target_col_missing].notna()]\n",
    "df_missing = df_C[df_C[target_col_missing].isna()]\n",
    "\n",
    "print(f\"\\nNumber of missing rows in '{target_col_missing}': {len(df_missing)}\")\n",
    "\n",
    "# Define features (all other columns) and target\n",
    "X_train = df_not_missing.drop(columns=[target_col_missing])\n",
    "y_train = df_not_missing[target_col_missing]\n",
    "\n",
    "# Prepare rows where target_col_missing is NaN\n",
    "X_pred = df_missing.drop(columns=[target_col_missing])\n",
    "\n",
    "# Fill remaining missing numeric values (excluding the target column) with medians\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_pred = X_pred.fillna(X_pred.median())\n",
    "\n",
    "# --- Choose one non-linear regression model ---\n",
    "# Option 1: K-Nearest Neighbors Regressor\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Option 2: Decision Tree Regressor (uncomment the next line to use it instead)\n",
    "# model = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "\n",
    "# Fit model on all available (non-missing) data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict missing BILL_AMT1 values\n",
    "predicted_values = model.predict(X_pred)\n",
    "\n",
    "# Replace missing values with predicted values\n",
    "df_C.loc[df_C[target_col_missing].isna(), target_col_missing] = predicted_values\n",
    "\n",
    "# Verify the imputation\n",
    "print(\"\\nMissing values after non-linear regression imputation:\")\n",
    "print(df_C[target_col_missing].isna().sum())\n",
    "\n",
    "print(\"\\nShape of Dataset C:\", df_C.shape)\n",
    "print(df_C[[target_col_missing]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de829cd",
   "metadata": {},
   "source": [
    "### Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1d3f030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Dataset D after listwise deletion: (28503, 25)\n",
      "\n",
      "‚úÖ Dataset A split complete:\n",
      "  Training set shape: (24000, 24)\n",
      "  Testing set shape:  (6000, 24)\n",
      "  Target distribution (train):\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "‚úÖ Dataset B split complete:\n",
      "  Training set shape: (24000, 24)\n",
      "  Testing set shape:  (6000, 24)\n",
      "  Target distribution (train):\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "‚úÖ Dataset C split complete:\n",
      "  Training set shape: (24000, 24)\n",
      "  Testing set shape:  (6000, 24)\n",
      "  Target distribution (train):\n",
      "default.payment.next.month\n",
      "0    0.779\n",
      "1    0.221\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "‚úÖ Dataset D split complete:\n",
      "  Training set shape: (22802, 24)\n",
      "  Testing set shape:  (5701, 24)\n",
      "  Target distribution (train):\n",
      "default.payment.next.month\n",
      "0    0.78\n",
      "1    0.22\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Split: For each of the three imputed datasets (A, B, C), split the data into training and testing sets. Also, create a fourth dataset (Dataset D) by simply removing all rows that contain any missing values (Listwise Deletion). Split Dataset D into training and testing sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Dataset D ‚Äî Listwise Deletion\n",
    "# ===============================\n",
    "\n",
    "# Drop all rows with any missing values\n",
    "df_D = df_mar_1.dropna()\n",
    "\n",
    "print(\"\\nShape of Dataset D after listwise deletion:\", df_D.shape)\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ Define target column\n",
    "# ===============================\n",
    "target_col = \"default.payment.next.month\"\n",
    "\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ Split each dataset into X (features) and y (target)\n",
    "# ===============================\n",
    "\n",
    "datasets = {\n",
    "    \"A\": df_A,\n",
    "    \"B\": df_B,\n",
    "    \"C\": df_C,\n",
    "    \"D\": df_D\n",
    "}\n",
    "\n",
    "splits = {}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    X = dataset.drop(columns=[target_col])\n",
    "    y = dataset[target_col]\n",
    "\n",
    "    # Perform 80/20 train-test split (use same random_state for reproducibility)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    splits[name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset {name} split complete:\")\n",
    "    print(f\"  Training set shape: {X_train.shape}\")\n",
    "    print(f\"  Testing set shape:  {X_test.shape}\")\n",
    "    print(f\"  Target distribution (train):\\n{y_train.value_counts(normalize=True).round(3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412c1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dataset A standardized successfully.\n",
      "  X_train_scaled shape: (24000, 24)\n",
      "  X_test_scaled shape:  (6000, 24)\n",
      "\n",
      "‚úÖ Dataset B standardized successfully.\n",
      "  X_train_scaled shape: (24000, 24)\n",
      "  X_test_scaled shape:  (6000, 24)\n",
      "\n",
      "‚úÖ Dataset C standardized successfully.\n",
      "  X_train_scaled shape: (24000, 24)\n",
      "  X_test_scaled shape:  (6000, 24)\n",
      "\n",
      "‚úÖ Dataset D standardized successfully.\n",
      "  X_train_scaled shape: (22802, 24)\n",
      "  X_test_scaled shape:  (5701, 24)\n"
     ]
    }
   ],
   "source": [
    "# 2. Classifier Setup: Standardize the features in all four datasets (A, B, C, D) using StandardScaler.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a dictionary to store scaled datasets\n",
    "scaled_splits = {}\n",
    "\n",
    "# Loop through each dataset (A, B, C, D)\n",
    "for name, split in splits.items():\n",
    "    X_train = split[\"X_train\"]\n",
    "    X_test = split[\"X_test\"]\n",
    "    y_train = split[\"y_train\"]\n",
    "    y_test = split[\"y_test\"]\n",
    "\n",
    "    # Initialize StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit scaler only on training features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Apply same transformation to testing features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Store scaled results in dictionary\n",
    "    scaled_splits[name] = {\n",
    "        \"X_train_scaled\": X_train_scaled,\n",
    "        \"X_test_scaled\": X_test_scaled,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"scaler\": scaler\n",
    "    }\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset {name} standardized successfully.\")\n",
    "    print(f\"  X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "    print(f\"  X_test_scaled shape:  {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "101857d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logistic Regression Model Performance Across Datasets\n",
      "\n",
      "===== Dataset A =====\n",
      "Accuracy: 0.808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.970     0.887      4673\n",
      "           1      0.693     0.240     0.356      1327\n",
      "\n",
      "    accuracy                          0.808      6000\n",
      "   macro avg      0.755     0.605     0.622      6000\n",
      "weighted avg      0.790     0.808     0.770      6000\n",
      "\n",
      "============================================================\n",
      "===== Dataset B =====\n",
      "Accuracy: 0.808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.969     0.887      4673\n",
      "           1      0.688     0.241     0.357      1327\n",
      "\n",
      "    accuracy                          0.808      6000\n",
      "   macro avg      0.753     0.605     0.622      6000\n",
      "weighted avg      0.789     0.808     0.770      6000\n",
      "\n",
      "============================================================\n",
      "===== Dataset C =====\n",
      "Accuracy: 0.808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.969     0.887      4673\n",
      "           1      0.688     0.240     0.356      1327\n",
      "\n",
      "    accuracy                          0.808      6000\n",
      "   macro avg      0.753     0.604     0.621      6000\n",
      "weighted avg      0.789     0.808     0.770      6000\n",
      "\n",
      "============================================================\n",
      "===== Dataset D =====\n",
      "Accuracy: 0.818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.823     0.977     0.894      4447\n",
      "           1      0.759     0.256     0.383      1254\n",
      "\n",
      "    accuracy                          0.818      5701\n",
      "   macro avg      0.791     0.617     0.638      5701\n",
      "weighted avg      0.809     0.818     0.781      5701\n",
      "\n",
      "============================================================\n",
      "\n",
      " Summary of Accuracy Across Datasets:\n",
      "Dataset A: 0.808\n",
      "Dataset B: 0.808\n",
      "Dataset C: 0.808\n",
      "Dataset D: 0.818\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Evaluation: Train a Logistic Regression classifier on the training set of each of the four datasets (A, B, C, D). Evaluate the performance of each model on its respective test set using a full Classification Report (Accuracy, Precision, Recall, F1-score).\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "print(\" Logistic Regression Model Performance Across Datasets\\n\")\n",
    "\n",
    "for name, data in scaled_splits.items():\n",
    "    X_train = data[\"X_train_scaled\"]\n",
    "    X_test = data[\"X_test_scaled\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "\n",
    "    # Initialize Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=3, output_dict=False)\n",
    "    report_dict = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"report_dict\": report_dict\n",
    "    }\n",
    "\n",
    "    print(f\"===== Dataset {name} =====\")\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(report)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Optional: summary comparison\n",
    "print(\"\\n Summary of Accuracy Across Datasets:\")\n",
    "for name, res in results.items():\n",
    "    print(f\"Dataset {name}: {res['accuracy']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e5d49",
   "metadata": {},
   "source": [
    "### Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2e516de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Logistic Regression Performance Summary (Datasets A‚ÄìD):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 (Class 0)</th>\n",
       "      <th>F1 (Class 1)</th>\n",
       "      <th>F1 (Weighted Avg)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model D</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model A</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model B</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model C</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Accuracy  F1 (Class 0)  F1 (Class 1)  F1 (Weighted Avg)  \\\n",
       "0  Model D     0.818         0.894         0.383              0.781   \n",
       "1  Model A     0.808         0.887         0.356              0.770   \n",
       "2  Model B     0.808         0.887         0.357              0.770   \n",
       "3  Model C     0.808         0.887         0.356              0.770   \n",
       "\n",
       "   Precision (Weighted)  Recall (Weighted)  \n",
       "0                 0.809              0.818  \n",
       "1                 0.790              0.808  \n",
       "2                 0.789              0.808  \n",
       "3                 0.789              0.808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a summary table comparing the performance metrics(especially F1-score) of the four models:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a summary table for comparison\n",
    "summary_data = []\n",
    "\n",
    "for name, res in results.items():\n",
    "    report = res['report_dict']\n",
    "    acc = res['accuracy']\n",
    "    \n",
    "    # Extract F1-score (for both classes and weighted average)\n",
    "    f1_default_0 = report['0']['f1-score']\n",
    "    f1_default_1 = report['1']['f1-score']\n",
    "    f1_weighted = report['weighted avg']['f1-score']\n",
    "    precision_weighted = report['weighted avg']['precision']\n",
    "    recall_weighted = report['weighted avg']['recall']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': f\"Model {name}\",\n",
    "        'Accuracy': round(acc, 3),\n",
    "        'F1 (Class 0)': round(f1_default_0, 3),\n",
    "        'F1 (Class 1)': round(f1_default_1, 3),\n",
    "        'F1 (Weighted Avg)': round(f1_weighted, 3),\n",
    "        'Precision (Weighted)': round(precision_weighted, 3),\n",
    "        'Recall (Weighted)': round(recall_weighted, 3)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Sort by weighted F1-score (optional)\n",
    "summary_df = summary_df.sort_values(by='F1 (Weighted Avg)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"üìã Logistic Regression Performance Summary (Datasets A‚ÄìD):\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bfd72",
   "metadata": {},
   "source": [
    "2. Efficacy Discussion\n",
    "\n",
    "| Aspect            | Models A/B/C (Imputation)           | Model D (Listwise Deletion)                                     |\n",
    "| ----------------- | ----------------------------------- | --------------------------------------------------------------- |\n",
    "| Data Retention    | 100% of rows retained               | Only 5701 rows (dropped ~5% with missing `BILL_AMT1`)           |\n",
    "| Accuracy          | 0.808                               | 0.818                                                           |\n",
    "| Weighted F1       | 0.770                               | 0.781                                                           |\n",
    "| Minority Class F1 | 0.356‚Äì0.357                         | 0.383                                                           |\n",
    "| Bias Risk         | Slight bias due to imputation error | Potential bias due to removed rows (MAR)                        |\n",
    "| Observations      | Robust, no lost data                | Slightly better metrics but less data; majority class dominates |\n",
    "\n",
    "\n",
    "- Even though imputation preserves more data, Listwise Deletion can sometimes appear better in overall metrics if the dropped rows include ‚Äúharder‚Äù examples.\n",
    "- Weighted metrics favor the majority class, so improvements in accuracy/F1 may reflect better performance on the majority class, not minority.\n",
    "- If the goal is to predict the minority class (default = 1), all models struggle ‚Äî class imbalance is the main challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
